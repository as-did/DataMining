{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac53e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ torch 已安装\n",
      "✓ transformers 已安装\n",
      "✓ pandas 已安装\n",
      "✓ numpy 已安装\n",
      "✗ scikit-learn 未安装\n",
      "✓ tqdm 已安装\n",
      "\n",
      "正在安装缺失的包: ['scikit-learn']\n",
      "✓ scikit-learn 安装成功\n",
      "\n",
      "环境检查完成！\n"
     ]
    }
   ],
   "source": [
    "# 1. 环境准备和导入检查\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 检查并安装必要的包\n",
    "def check_and_install():\n",
    "    required_packages = {\n",
    "        'torch': 'torch',\n",
    "        'transformers': 'transformers',\n",
    "        'pandas': 'pandas',\n",
    "        'numpy': 'numpy',\n",
    "        'scikit-learn': 'scikit-learn',\n",
    "        'tqdm': 'tqdm'\n",
    "    }\n",
    "    \n",
    "    missing_packages = []\n",
    "    for package, import_name in required_packages.items():\n",
    "        try:\n",
    "            importlib.import_module(import_name)\n",
    "            print(f\"✓ {package} 已安装\")\n",
    "        except ImportError:\n",
    "            print(f\"✗ {package} 未安装\")\n",
    "            missing_packages.append(package)\n",
    "    \n",
    "    if missing_packages:\n",
    "        print(f\"\\n正在安装缺失的包: {missing_packages}\")\n",
    "        for package in missing_packages:\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "                print(f\"✓ {package} 安装成功\")\n",
    "            except subprocess.CalledProcessError:\n",
    "                print(f\"✗ {package} 安装失败\")\n",
    "    \n",
    "    print(\"\\n环境检查完成！\")\n",
    "\n",
    "check_and_install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74e2951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 成功导入transformers核心模块\n",
      "✓ 从torch.optim导入AdamW\n",
      "✓ 导入get_linear_schedule_with_warmup\n",
      "\n",
      "设备信息:\n",
      "- 使用设备: cpu\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. 基础设置和导入\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 检查transformers版本并调整导入\n",
    "try:\n",
    "    # 尝试从transformers导入所有需要的模块\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    print(\"✓ 成功导入transformers核心模块\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ transformers导入失败: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 尝试不同的方式导入AdamW\n",
    "try:\n",
    "    # 方式1：从transformers.optimization导入\n",
    "    from transformers.optimization import AdamW\n",
    "    print(\"✓ 从transformers.optimization导入AdamW\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        # 方式2：从torch.optim导入\n",
    "        from torch.optim import AdamW\n",
    "        print(\"✓ 从torch.optim导入AdamW\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            # 方式3：直接使用transformers中的AdamW\n",
    "            from transformers import AdamW\n",
    "            print(\"✓ 直接导入AdamW\")\n",
    "        except ImportError:\n",
    "            print(\"✗ 无法找到AdamW，将使用torch.optim.Adam\")\n",
    "            AdamW = None\n",
    "\n",
    "# 导入scheduler\n",
    "try:\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    print(\"✓ 导入get_linear_schedule_with_warmup\")\n",
    "except ImportError:\n",
    "    print(\"✗ 无法导入get_linear_schedule_with_warmup\")\n",
    "    get_linear_schedule_with_warmup = None\n",
    "\n",
    "# 设置随机种子\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n设备信息:\")\n",
    "print(f\"- 使用设备: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"- GPU名称: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"- GPU显存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2e5e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SentimentDataset类定义完成\n"
     ]
    }
   ],
   "source": [
    "# 3. 数据处理类\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=128, has_header=False):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        Args:\n",
    "            file_path: CSV文件路径\n",
    "            tokenizer: 分词器\n",
    "            max_length: 最大序列长度\n",
    "            has_header: CSV文件是否有表头\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"\\n加载数据集: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # 尝试读取CSV文件\n",
    "            if has_header:\n",
    "                df = pd.read_csv(file_path)\n",
    "                # 检查列数\n",
    "                if len(df.columns) == 3:\n",
    "                    df.columns = ['polarity', 'title', 'text']\n",
    "                else:\n",
    "                    print(f\"警告: 期望3列，但找到{len(df.columns)}列\")\n",
    "                    # 尝试自动分配列名\n",
    "                    if len(df.columns) >= 3:\n",
    "                        df = df.iloc[:, :3]\n",
    "                        df.columns = ['polarity', 'title', 'text']\n",
    "                    else:\n",
    "                        raise ValueError(f\"数据列数不足: {len(df.columns)}\")\n",
    "            else:\n",
    "                # 没有表头，直接读取并指定列名\n",
    "                df = pd.read_csv(file_path, header=None)\n",
    "                \n",
    "                # 检查列数\n",
    "                if df.shape[1] == 3:\n",
    "                    df.columns = ['polarity', 'title', 'text']\n",
    "                elif df.shape[1] > 3:\n",
    "                    print(f\"警告: 数据有{df.shape[1]}列，只取前3列\")\n",
    "                    df = df.iloc[:, :3]\n",
    "                    df.columns = ['polarity', 'title', 'text']\n",
    "                else:\n",
    "                    raise ValueError(f\"数据列数不足: {df.shape[1]}\")\n",
    "            \n",
    "            # 显示数据基本信息\n",
    "            print(f\"- 原始数据行数: {len(df)}\")\n",
    "            \n",
    "            # 数据预处理\n",
    "            # 处理缺失值\n",
    "            df['title'] = df['title'].fillna('')\n",
    "            df['text'] = df['text'].fillna('')\n",
    "            \n",
    "            # 合并title和text\n",
    "            df['combined_text'] = df['title'].astype(str) + \" \" + df['text'].astype(str)\n",
    "            \n",
    "            # 检查极性值的有效性\n",
    "            valid_polarity = df['polarity'].isin([1, 2])\n",
    "            if not valid_polarity.all():\n",
    "                invalid_count = (~valid_polarity).sum()\n",
    "                print(f\"警告: 发现{invalid_count}个无效的极性值，将被移除\")\n",
    "                df = df[valid_polarity].copy()\n",
    "            \n",
    "            # 标签映射：1(负面)->0, 2(正面)->1\n",
    "            df['label'] = df['polarity'].apply(lambda x: 0 if x == 1 else 1)\n",
    "            \n",
    "            self.texts = df['combined_text'].tolist()\n",
    "            self.labels = df['label'].tolist()\n",
    "            \n",
    "            print(f\"- 有效数据行数: {len(self.labels)}\")\n",
    "            print(f\"- 标签分布: 负面(0): {self.labels.count(0)}, 正面(1): {self.labels.count(1)}\")\n",
    "            \n",
    "            # 显示前3个样本\n",
    "            print(\"- 样本示例:\")\n",
    "            for i in range(min(3, len(self.labels))):\n",
    "                print(f\"  样本{i+1}: 文本长度={len(self.texts[i])}, 标签={self.labels[i]}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"错误: 文件 {file_path} 未找到\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"错误: 读取文件时出错 - {e}\")\n",
    "            raise\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 分词\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"✓ SentimentDataset类定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58451677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 训练和评估函数定义完成\n"
     ]
    }
   ],
   "source": [
    "# 4. 训练和评估函数\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"训练\")\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # 获取预测结果\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # 更新进度条\n",
    "        if batch_idx % 10 == 0:\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "def eval_model(model, data_loader, device, dataset_name=\"验证集\"):\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(data_loader, desc=f\"评估{dataset_name}\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{dataset_name}分类报告:\")\n",
    "    print(classification_report(all_labels, all_preds, \n",
    "                                target_names=['负面', '正面'],\n",
    "                                digits=4))\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels\n",
    "\n",
    "print(\"✓ 训练和评估函数定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33297562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 主训练流程定义完成（已添加采样功能）\n"
     ]
    }
   ],
   "source": [
    "# 5. 主训练流程\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"开始情感分类训练（使用采样数据）\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 配置参数\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 8  # 较小的批次大小以适应显存\n",
    "    EPOCHS = 2  # 减少epochs以快速测试\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WARMUP_STEPS = 100  # 减少warmup步数\n",
    "    \n",
    "    # 数据采样配置\n",
    "    TRAIN_MAX_SAMPLES = 10000  # 训练集最多使用10000个样本\n",
    "    DEV_MAX_SAMPLES = 1000     # 验证集最多使用1000个样本\n",
    "    TEST_MAX_SAMPLES = 1000    # 测试集最多使用1000个样本\n",
    "    \n",
    "    print(f\"\\n模型配置:\")\n",
    "    print(f\"- 模型: {MODEL_NAME}\")\n",
    "    print(f\"- 最大长度: {MAX_LENGTH}\")\n",
    "    print(f\"- 批次大小: {BATCH_SIZE}\")\n",
    "    print(f\"- 训练轮数: {EPOCHS}\")\n",
    "    print(f\"- 学习率: {LEARNING_RATE}\")\n",
    "    print(f\"- 训练集最大样本数: {TRAIN_MAX_SAMPLES}\")\n",
    "    print(f\"- 验证集最大样本数: {DEV_MAX_SAMPLES}\")\n",
    "    print(f\"- 测试集最大样本数: {TEST_MAX_SAMPLES}\")\n",
    "    \n",
    "    # 检查数据文件\n",
    "    data_files = [\"train.csv\", \"dev.csv\", \"test.csv\"]\n",
    "    missing_files = []\n",
    "    \n",
    "    print(f\"\\n检查数据文件:\")\n",
    "    for file in data_files:\n",
    "        if os.path.exists(file):\n",
    "            # 检查文件行数\n",
    "            try:\n",
    "                with open(file, 'r', encoding='utf-8') as f:\n",
    "                    line_count = sum(1 for line in f)\n",
    "                file_size = os.path.getsize(file) / 1024 / 1024  # MB\n",
    "                print(f\"✓ {file}: {line_count} 行, {file_size:.2f} MB\")\n",
    "            except:\n",
    "                file_size = os.path.getsize(file) / 1024 / 1024  # MB\n",
    "                print(f\"✓ {file}: {file_size:.2f} MB (无法统计行数)\")\n",
    "        else:\n",
    "            print(f\"✗ {file}: 未找到\")\n",
    "            missing_files.append(file)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"\\n警告: 缺少以下数据文件: {missing_files}\")\n",
    "        print(\"请确保文件在当前目录下\")\n",
    "        \n",
    "        # 创建小规模测试数据\n",
    "        create_test_data = input(\"是否创建测试数据? (y/n): \").lower()\n",
    "        if create_test_data == 'y':\n",
    "            create_test_datasets()\n",
    "            print(\"测试数据已创建\")\n",
    "        else:\n",
    "            print(\"请手动准备数据文件后重新运行\")\n",
    "            return\n",
    "    \n",
    "    print(f\"\\n加载分词器: {MODEL_NAME}\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        # 设置填充token\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        print(\"✓ 分词器加载成功\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ 分词器加载失败: {e}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n加载模型: {MODEL_NAME}\")\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=2,\n",
    "            problem_type=\"single_label_classification\"\n",
    "        )\n",
    "        \n",
    "        # 设置模型填充token id\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        model = model.to(device)\n",
    "        print(\"✓ 模型加载成功\")\n",
    "        \n",
    "        # 打印模型信息\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"- 总参数量: {total_params:,}\")\n",
    "        print(f\"- 可训练参数量: {trainable_params:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ 模型加载失败: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 加载数据集（使用采样）\n",
    "    print(f\"\\n加载数据集（使用采样）...\")\n",
    "    try:\n",
    "        train_dataset = SentimentDataset(\n",
    "            \"train.csv\", \n",
    "            tokenizer, \n",
    "            MAX_LENGTH, \n",
    "            has_header=False,\n",
    "            max_samples=TRAIN_MAX_SAMPLES\n",
    "        )\n",
    "        dev_dataset = SentimentDataset(\n",
    "            \"dev.csv\", \n",
    "            tokenizer, \n",
    "            MAX_LENGTH, \n",
    "            has_header=False,\n",
    "            max_samples=DEV_MAX_SAMPLES\n",
    "        )\n",
    "        test_dataset = SentimentDataset(\n",
    "            \"test.csv\", \n",
    "            tokenizer, \n",
    "            MAX_LENGTH, \n",
    "            has_header=False,\n",
    "            max_samples=TEST_MAX_SAMPLES\n",
    "        )\n",
    "        \n",
    "        print(\"✓ 数据集加载成功（已采样）\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ 数据集加载失败: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"\\n数据加载器创建完成:\")\n",
    "    print(f\"- 训练集批次: {len(train_loader)} (共{len(train_dataset)}个样本)\")\n",
    "    print(f\"- 验证集批次: {len(dev_loader)} (共{len(dev_dataset)}个样本)\")\n",
    "    print(f\"- 测试集批次: {len(test_loader)} (共{len(test_dataset)}个样本)\")\n",
    "    \n",
    "    # 配置优化器\n",
    "    print(f\"\\n配置优化器...\")\n",
    "    try:\n",
    "        if AdamW is not None:\n",
    "            optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "            print(\"✓ 使用AdamW优化器\")\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "            print(\"✓ 使用Adam优化器\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ 优化器配置失败: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 配置学习率调度器\n",
    "    scheduler = None\n",
    "    if get_linear_schedule_with_warmup:\n",
    "        total_steps = len(train_loader) * EPOCHS\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=WARMUP_STEPS,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        print(\"✓ 学习率调度器配置成功\")\n",
    "    else:\n",
    "        print(\"⚠ 无法配置学习率调度器，将不使用warmup\")\n",
    "    \n",
    "    # 训练循环\n",
    "    print(f\"\\n开始训练循环...\")\n",
    "    best_f1 = 0\n",
    "    best_model_path = \"best_qwen_sentiment_model\"\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # 训练\n",
    "        train_loss, train_acc, train_f1 = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, device\n",
    "        )\n",
    "        print(f\"训练结果 - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
    "        \n",
    "        # 在验证集上评估\n",
    "        print(f\"\\n在验证集上评估...\")\n",
    "        dev_loss, dev_acc, dev_f1, _, _ = eval_model(model, dev_loader, device, \"验证集\")\n",
    "        print(f\"验证集结果 - Loss: {dev_loss:.4f}, Accuracy: {dev_acc:.4f}, F1: {dev_f1:.4f}\")\n",
    "        \n",
    "        # 保存历史记录\n",
    "        history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'train_f1': train_f1,\n",
    "            'dev_loss': dev_loss,\n",
    "            'dev_acc': dev_acc,\n",
    "            'dev_f1': dev_f1\n",
    "        })\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if dev_f1 > best_f1:\n",
    "            best_f1 = dev_f1\n",
    "            try:\n",
    "                model.save_pretrained(best_model_path)\n",
    "                tokenizer.save_pretrained(best_model_path)\n",
    "                print(f\"✓ 保存最佳模型到 {best_model_path}，F1分数: {best_f1:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ 保存模型失败: {e}\")\n",
    "    \n",
    "    # 保存训练历史\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df.to_csv('training_history.csv', index=False)\n",
    "    print(f\"\\n✓ 训练历史已保存到 training_history.csv\")\n",
    "    \n",
    "    # 测试最佳模型\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"测试最佳模型\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        # 加载最佳模型\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
    "        best_model = best_model.to(device)\n",
    "        print(\"✓ 最佳模型加载成功\")\n",
    "        \n",
    "        # 在测试集上评估\n",
    "        test_loss, test_acc, test_f1, test_preds, test_labels = eval_model(\n",
    "            best_model, test_loader, device, \"测试集\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n最终测试结果:\")\n",
    "        print(f\"- Loss: {test_loss:.4f}\")\n",
    "        print(f\"- Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"- F1 Score: {test_f1:.4f}\")\n",
    "        \n",
    "        # 保存预测结果\n",
    "        test_results = pd.DataFrame({\n",
    "            '真实标签': test_labels,\n",
    "            '预测标签': test_preds,\n",
    "            '预测正确': [1 if pred == true else 0 for pred, true in zip(test_preds, test_labels)]\n",
    "        })\n",
    "        test_results.to_csv('test_predictions.csv', index=False)\n",
    "        print(f\"\\n✓ 预测结果已保存到 test_predictions.csv\")\n",
    "        \n",
    "        # 保存最终模型\n",
    "        final_model_path = \"final_qwen_sentiment_model\"\n",
    "        best_model.save_pretrained(final_model_path)\n",
    "        tokenizer.save_pretrained(final_model_path)\n",
    "        print(f\"✓ 最终模型已保存到 {final_model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ 测试最佳模型失败: {e}\")\n",
    "\n",
    "print(\"✓ 主训练流程定义完成（已添加采样功能）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9987fdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 辅助函数定义完成\n",
      "准备运行主程序...\n",
      "\n",
      "创建测试数据集...\n",
      "✓ 测试数据集创建完成:\n",
      "  - train.csv: 1000 行\n",
      "  - dev.csv: 100 行\n",
      "  - test.csv: 100 行\n",
      "\n",
      "============================================================\n",
      "开始情感分类训练（使用采样数据）\n",
      "============================================================\n",
      "\n",
      "模型配置:\n",
      "- 模型: Qwen/Qwen2.5-0.5B\n",
      "- 最大长度: 128\n",
      "- 批次大小: 8\n",
      "- 训练轮数: 2\n",
      "- 学习率: 2e-05\n",
      "- 训练集最大样本数: 10000\n",
      "- 验证集最大样本数: 1000\n",
      "- 测试集最大样本数: 1000\n",
      "\n",
      "检查数据文件:\n",
      "✓ train.csv: 1000 行, 0.05 MB\n",
      "✓ dev.csv: 100 行, 0.00 MB\n",
      "✓ test.csv: 100 行, 0.00 MB\n",
      "\n",
      "加载分词器: Qwen/Qwen2.5-0.5B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af16de4aebe743f589ded9e424b7bb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9164a5b7a7bd45f6b27e26080e1aae16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7838e320e68a441fb55b435241afa1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725fb436fa2d42d49d07f576c92fe181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 分词器加载成功\n",
      "\n",
      "加载模型: Qwen/Qwen2.5-0.5B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfc6d9ad83a431bab02835aa7daaeee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adec0d87dc548be910c383072a69d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 模型加载成功\n",
      "- 总参数量: 494,034,560\n",
      "- 可训练参数量: 494,034,560\n",
      "\n",
      "加载数据集（使用采样）...\n",
      "\n",
      "加载数据集: train.csv\n",
      "- 原始数据行数: 1000\n",
      "- 有效数据行数: 1000\n",
      "- 标签分布: 负面(0): 500, 正面(1): 500\n",
      "- 样本示例:\n",
      "  样本1: 文本='负面评论 质量很差，不推荐购买。', 标签=0\n",
      "  样本2: 文本='正面评论 质量很好，强烈推荐！', 标签=1\n",
      "  样本3: 文本='负面评论 质量很差，不推荐购买。', 标签=0\n",
      "\n",
      "加载数据集: dev.csv\n",
      "- 原始数据行数: 100\n",
      "- 有效数据行数: 100\n",
      "- 标签分布: 负面(0): 50, 正面(1): 50\n",
      "- 样本示例:\n",
      "  样本1: 文本='测试负面 这是一个测试评论。', 标签=0\n",
      "  样本2: 文本='测试正面 这是一个测试评论。', 标签=1\n",
      "  样本3: 文本='测试负面 这是一个测试评论。', 标签=0\n",
      "\n",
      "加载数据集: test.csv\n",
      "- 原始数据行数: 100\n",
      "- 有效数据行数: 100\n",
      "- 标签分布: 负面(0): 50, 正面(1): 50\n",
      "- 样本示例:\n",
      "  样本1: 文本='测试负面 这是一个测试评论。', 标签=0\n",
      "  样本2: 文本='测试正面 这是一个测试评论。', 标签=1\n",
      "  样本3: 文本='测试负面 这是一个测试评论。', 标签=0\n",
      "✓ 数据集加载成功（已采样）\n",
      "\n",
      "数据加载器创建完成:\n",
      "- 训练集批次: 125 (共1000个样本)\n",
      "- 验证集批次: 13 (共100个样本)\n",
      "- 测试集批次: 13 (共100个样本)\n",
      "\n",
      "配置优化器...\n",
      "✓ 使用AdamW优化器\n",
      "✓ 学习率调度器配置成功\n",
      "\n",
      "开始训练循环...\n",
      "\n",
      "==================================================\n",
      "Epoch 1/2\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练: 100%|██████████| 125/125 [09:23<00:00,  4.51s/it, loss=0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练结果 - Loss: 0.1812, Accuracy: 0.9750, F1: 0.9750\n",
      "\n",
      "在验证集上评估...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "评估验证集: 100%|██████████| 13/13 [00:13<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "验证集分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          负面     0.5000    1.0000    0.6667        50\n",
      "          正面     0.0000    0.0000    0.0000        50\n",
      "\n",
      "    accuracy                         0.5000       100\n",
      "   macro avg     0.2500    0.5000    0.3333       100\n",
      "weighted avg     0.2500    0.5000    0.3333       100\n",
      "\n",
      "验证集结果 - Loss: 9.3914, Accuracy: 0.5000, F1: 0.3333\n",
      "✓ 保存最佳模型到 best_qwen_sentiment_model，F1分数: 0.3333\n",
      "\n",
      "==================================================\n",
      "Epoch 2/2\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练: 100%|██████████| 125/125 [08:02<00:00,  3.86s/it, loss=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练结果 - Loss: 0.0000, Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "在验证集上评估...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "评估验证集: 100%|██████████| 13/13 [00:13<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "验证集分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          负面     0.5000    1.0000    0.6667        50\n",
      "          正面     0.0000    0.0000    0.0000        50\n",
      "\n",
      "    accuracy                         0.5000       100\n",
      "   macro avg     0.2500    0.5000    0.3333       100\n",
      "weighted avg     0.2500    0.5000    0.3333       100\n",
      "\n",
      "验证集结果 - Loss: 9.3921, Accuracy: 0.5000, F1: 0.3333\n",
      "\n",
      "✓ 训练历史已保存到 training_history.csv\n",
      "\n",
      "==================================================\n",
      "测试最佳模型\n",
      "==================================================\n",
      "✓ 最佳模型加载成功\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "评估测试集: 100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试集分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          负面     0.5000    1.0000    0.6667        50\n",
      "          正面     0.0000    0.0000    0.0000        50\n",
      "\n",
      "    accuracy                         0.5000       100\n",
      "   macro avg     0.2500    0.5000    0.3333       100\n",
      "weighted avg     0.2500    0.5000    0.3333       100\n",
      "\n",
      "\n",
      "最终测试结果:\n",
      "- Loss: 9.3914\n",
      "- Accuracy: 0.5000\n",
      "- F1 Score: 0.3333\n",
      "\n",
      "✓ 预测结果已保存到 test_predictions.csv\n",
      "✓ 最终模型已保存到 final_qwen_sentiment_model\n",
      "\n",
      "==================================================\n",
      "测试情感预测\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_qwen_sentiment_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试 1:\n",
      "文本: 这个产品质量非常好，我非常满意！\n",
      "情感: 正面 (标签: 1)\n",
      "置信度: 1.0000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_qwen_sentiment_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试 2:\n",
      "文本: 糟糕的体验，不会再购买了。\n",
      "情感: 负面 (标签: 0)\n",
      "置信度: 1.0000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_qwen_sentiment_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试 3:\n",
      "文本: 一般般，没什么特别的感觉，凑合能用。\n",
      "情感: 正面 (标签: 1)\n",
      "置信度: 0.5361\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_qwen_sentiment_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试 4:\n",
      "文本: 物超所值，性价比很高，推荐给大家。\n",
      "情感: 正面 (标签: 1)\n",
      "置信度: 0.9924\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_qwen_sentiment_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试 5:\n",
      "文本: 客服态度差，解决问题效率低。\n",
      "情感: 负面 (标签: 0)\n",
      "置信度: 1.0000\n",
      "----------------------------------------\n",
      "\n",
      "==================================================\n",
      "程序执行完成！\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 6. 辅助函数和测试\n",
    "def create_test_datasets():\n",
    "    \"\"\"创建测试数据集\"\"\"\n",
    "    print(\"\\n创建测试数据集...\")\n",
    "    \n",
    "    # 创建小规模训练数据（1000行，用于测试）\n",
    "    train_data = {\n",
    "        'polarity': [1, 2] * 500,  # 1000行\n",
    "        'title': ['负面评论' if i % 2 == 0 else '正面评论' for i in range(1000)],\n",
    "        'text': ['质量很差，不推荐购买。' if i % 2 == 0 else '质量很好，强烈推荐！' for i in range(1000)]\n",
    "    }\n",
    "    \n",
    "    # 创建验证和测试数据（各100行）\n",
    "    test_data = {\n",
    "        'polarity': [1, 2] * 50,  # 100行\n",
    "        'title': ['测试负面' if i % 2 == 0 else '测试正面' for i in range(100)],\n",
    "        'text': ['这是一个测试评论。' for _ in range(100)]\n",
    "    }\n",
    "    \n",
    "    # 保存为CSV文件（无表头）\n",
    "    pd.DataFrame(train_data).to_csv('train.csv', index=False, header=False)\n",
    "    pd.DataFrame(test_data).to_csv('dev.csv', index=False, header=False)\n",
    "    pd.DataFrame(test_data).to_csv('test.csv', index=False, header=False)\n",
    "    \n",
    "    print(\"✓ 测试数据集创建完成:\")\n",
    "    print(f\"  - train.csv: {len(train_data['polarity'])} 行\")\n",
    "    print(f\"  - dev.csv: {len(test_data['polarity'])} 行\")\n",
    "    print(f\"  - test.csv: {len(test_data['polarity'])} 行\")\n",
    "\n",
    "def predict_sentiment(text, model_path=\"final_qwen_sentiment_model\", max_length=128):\n",
    "    \"\"\"使用训练好的模型预测文本情感\"\"\"\n",
    "    try:\n",
    "        # 加载模型和分词器\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # 预处理文本\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # 预测\n",
    "        with torch.no_grad():\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            \n",
    "            # 将0/1映射回原始标签\n",
    "            sentiment_label = predictions.item()\n",
    "            sentiment_text = \"负面\" if sentiment_label == 0 else \"正面\"\n",
    "            \n",
    "            # 计算置信度\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            confidence = probabilities[0][sentiment_label].item()\n",
    "        \n",
    "        return sentiment_text, confidence, sentiment_label\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"预测失败: {e}\")\n",
    "        return \"未知\", 0.0, -1\n",
    "\n",
    "def test_prediction():\n",
    "    \"\"\"测试预测函数\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"测试情感预测\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    test_texts = [\n",
    "        \"这个产品质量非常好，我非常满意！\",\n",
    "        \"糟糕的体验，不会再购买了。\",\n",
    "        \"一般般，没什么特别的感觉，凑合能用。\",\n",
    "        \"物超所值，性价比很高，推荐给大家。\",\n",
    "        \"客服态度差，解决问题效率低。\"\n",
    "    ]\n",
    "    \n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        sentiment, confidence, label = predict_sentiment(text)\n",
    "        print(f\"\\n测试 {i}:\")\n",
    "        print(f\"文本: {text}\")\n",
    "        print(f\"情感: {sentiment} (标签: {label})\")\n",
    "        print(f\"置信度: {confidence:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "print(\"✓ 辅助函数定义完成\")\n",
    "\n",
    "# 运行测试\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"准备运行主程序...\")\n",
    "    \n",
    "    # 首先测试创建数据\n",
    "    test_create = input(\"是否创建测试数据? (y/n): \").lower()\n",
    "    if test_create == 'y':\n",
    "        create_test_datasets()\n",
    "    \n",
    "    # 运行主训练\n",
    "    run_main = input(\"是否开始训练? (y/n): \").lower()\n",
    "    if run_main == 'y':\n",
    "        main()\n",
    "    \n",
    "    # 测试预测\n",
    "    test_predict = input(\"是否测试预测功能? (y/n): \").lower()\n",
    "    if test_predict == 'y':\n",
    "        test_prediction()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"程序执行完成！\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
