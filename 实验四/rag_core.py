import streamlit as st
import torch
import time
import re
from config import MAX_NEW_TOKENS_GEN, TEMPERATURE, TOP_P, REPETITION_PENALTY, QUERY_PREPROCESSING_MAX_TOKENS, \
    QUERY_PREPROCESSING_TEMPERATURE


def extract_medical_keywords(processed_query):
    """
    åŸºäºåŒ»å­¦è¯å…¸çš„è¯­ä¹‰å…³é”®è¯æå–
    è¯†åˆ«çœŸæ­£çš„åŒ»å­¦æœ¯è¯­ï¼Œè€Œéç®€å•çš„å­—ç¬¦ä¸²å·®é›†
    """
    if not processed_query:
        return []

    # å¸¸è§åŒ»å­¦æœ¯è¯­åç¼€æ¨¡å¼
    medical_suffixes = r'(ç‚|ç—‡|ç—…|ç˜¤|ç™Œ|ç—‡|å¾|ç•¸å½¢|æŸä¼¤|æ„ŸæŸ“|éšœç¢|åŠŸèƒ½ä¸å…¨|è¡°ç«­|å‡ºè¡€|æ¢—æ­»|æ “å¡|ç—›|ç—’|è‚¿|èƒ€|æ™•|éº»)'

    # å¸¸è§å‰ç¼€æ¨¡å¼
    medical_prefixes = r'(è¶…|äºš|æ€¥|æ…¢|ç‰¹|åŸ|ç»§|å|å†)'

    words = processed_query.split()
    keywords = []

    for word in words:
        # è§„åˆ™1ï¼šé•¿åº¦å¤§äº3ä¸”åŒ…å«åŒ»å­¦åç¼€
        if len(word) > 3 and re.search(medical_suffixes, word):
            keywords.append(word)
        # è§„åˆ™2ï¼šåŒ…å«å¸¸è§å‰ç¼€+åŒ»å­¦åç¼€ï¼ˆå¦‚"è¶…æ•ååº”"ï¼‰
        elif re.match(medical_prefixes + r'.*' + medical_suffixes, word):
            keywords.append(word)
        # è§„åˆ™3ï¼šçº¯è‹±æ–‡åŒ»å­¦æœ¯è¯­ï¼ˆå¦‚"DNA"ã€"RNA"ï¼‰
        elif re.match(r'^[A-Z]{2,}$', word):
            keywords.append(word)
        # è§„åˆ™4ï¼šå¸¸è§åŒ»å­¦æ“ä½œè¯
        elif word in ['è¯Šæ–­', 'æ²»ç–—', 'é¢„é˜²', 'æ£€æŸ¥', 'æ‰‹æœ¯', 'è¯ç‰©']:
            keywords.append(word)

    return keywords[:6]  # æœ€å¤šè¿”å›6ä¸ªå…³é”®è¯


def preprocess_query(user_input, gen_model, tokenizer):
    """
    å¢å¼ºç‰ˆæŸ¥è¯¢é¢„å¤„ç†ï¼šå¸¦å¼ºåˆ¶ä¿¡æ¯ä¿ç•™å’Œå¤šå±‚éªŒè¯

    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. Promptæ˜ç¡®ç¦æ­¢ç”Ÿæˆé€šç”¨å»ºè®®
    2. å¼ºåˆ¶ä¿ç•™åŸå§‹å…³é”®ä¿¡æ¯
    3. 4å±‚è¾“å‡ºéªŒè¯ï¼ˆå…³é”®è¯ä¿ç•™ã€é•¿åº¦ã€è¯­ä¹‰ã€é»‘åå•ï¼‰
    4. æ¨¡å‹å¤±è´¥æ—¶ç«‹å³å›é€€åˆ°å¯é çš„è§„åˆ™å¤„ç†
    """
    if not gen_model or not tokenizer:
        return rule_based_preprocess(user_input)  # ç›´æ¥å›é€€

    # ========== å…³é”®æ”¹è¿›1ï¼šå¼ºåˆ¶ä¿ç•™åŸå§‹ä¿¡æ¯çš„Prompt ==========
    prompt = f"""ä½œä¸ºåŒ»å­¦AIæ£€ç´¢åŠ©æ‰‹ï¼Œè¯·ä¼˜åŒ–ä»¥ä¸‹æŸ¥è¯¢ä»¥æé«˜æ£€ç´¢å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒè¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š**
1.  **å¿…é¡»ä¿ç•™**  æ‰€æœ‰åŸå§‹å…³é”®ä¿¡æ¯ï¼ˆç–¾ç—…ã€ç—‡çŠ¶ã€è¯ç‰©ã€æ²»ç–—æ–¹å¼ç­‰ï¼‰
2. **å¿…é¡»è½¬æ¢**å£è¯­åŒ–ä¸ºä¸“ä¸šåŒ»å­¦æœ¯è¯­ï¼ˆå¦‚"é¼»å­å µ"â†’"é¼»å¡"ï¼‰
3. **å¯ä»¥è¡¥å……**ç›¸å…³åŒ»å­¦ç»´åº¦ï¼ˆè¯Šæ–­ã€ç—…å› ã€é¢„é˜²ç­‰ï¼‰
4.  **ç¦æ­¢åˆ é™¤**  ä»»ä½•åŸå§‹ä¿¡æ¯æˆ–ç”Ÿæˆé€šç”¨å»ºè®®
5. **å¿…é¡»è¾“å‡º**ä¸“ä¸šåŒ»å­¦æŸ¥è¯¢ï¼Œä¸èƒ½æ˜¯é€šç”¨å›ç­”

**åˆæ ¼ç¤ºä¾‹ï¼š**
åŸå§‹ï¼š"é¼»å­å µäº†ï¼Œè¯¥åƒä»€ä¹ˆè¯ï¼Ÿ"
ä¼˜åŒ–ï¼š"é¼»å¡ è¯ç‰©æ²»ç–—" âœ“ï¼ˆä¿ç•™äº†é¼»å¡å’Œç”¨è¯ï¼‰

**å¤±è´¥ç¤ºä¾‹ï¼š**
åŸå§‹ï¼š"é¼»å­å µäº†ï¼Œè¯¥åƒä»€ä¹ˆè¯ï¼Ÿ"
ä¼˜åŒ–ï¼š"åŒ»ç”Ÿå»ºè®®åƒç‚¹ä»€ä¹ˆ" âŒï¼ˆä¸¢å¤±äº†æ‰€æœ‰å…³é”®ä¿¡æ¯ï¼‰

**å¤±è´¥ç¤ºä¾‹ï¼š**
åŸå§‹ï¼š"é¼»å­å µäº†ï¼Œè¯¥åƒä»€ä¹ˆè¯ï¼Ÿ"
ä¼˜åŒ–ï¼š"é¼»å¡" âŒï¼ˆä¸¢å¤±äº†"è¯ç‰©æ²»ç–—"ä¿¡æ¯ï¼‰

**è½¬æ¢è§„åˆ™ï¼š**
- é¼»å­å µ/é¼»å¡ â†’ é¼»å¡
- åƒä»€ä¹ˆè¯/ç”¨è¯ â†’ è¯ç‰©æ²»ç–—
- é¼»ç‚/é¼»çª¦ç‚ â†’ é¼»ç‚

è¯·ä¼˜åŒ–ä»¥ä¸‹æŸ¥è¯¢ï¼ˆåªè¾“å‡ºä¼˜åŒ–ç»“æœï¼Œä¸è§£é‡Šï¼‰ï¼š
åŸå§‹ï¼š"user_input"
ä¼˜åŒ–ï¼š"é¼»å¡ è¯ç‰©æ²»ç–— é¼»ç‚"  # ç¤ºä¾‹æ ¼å¼
---
åŸå§‹ï¼š"{user_input}"
ä¼˜åŒ–ï¼š
""".replace('user_input', user_input)  # ç¡®ä¿å˜é‡æ­£ç¡®æ’å…¥

    try:
        inputs = tokenizer(prompt, return_tensors="pt").to(gen_model.device)
        input_length = inputs['input_ids'].shape[1]

        # ç”Ÿæˆå‚æ•°ï¼šè°ƒæ•´ä¸ºæ›´ä¿å®ˆçš„è¾“å‡º
        generation_kwargs = {
            "max_new_tokens": QUERY_PREPROCESSING_MAX_TOKENS,
            "temperature": QUERY_PREPROCESSING_TEMPERATURE,  # ä½¿ç”¨æ›´ä½æ¸©åº¦
            "top_p": 0.95,
            "do_sample": False,
            "pad_token_id": tokenizer.eos_token_id,
        }

        with torch.no_grad():
            outputs = gen_model.generate(**inputs, **generation_kwargs)

        processed_query = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()


        # æ¸…ç†ï¼šåªä¿ç•™ç¬¬ä¸€è¡Œï¼Œå¹¶ç§»é™¤å¯èƒ½çš„æ ‡ç­¾
        if '\n' in processed_query:
            processed_query = processed_query.split('\n')[0].strip()

        # ç§»é™¤å¯èƒ½ç”Ÿæˆçš„æ ‡ç­¾
        processed_query = processed_query.replace('ä¼˜åŒ–ï¼š', '').replace('ç»“æœï¼š', '').strip()

        # ========== å…³é”®æ”¹è¿›3ï¼š4å±‚è¾“å‡ºè´¨é‡éªŒè¯ ==========

        # éªŒè¯1ï¼šå¿…é¡»åŒ…å«åŸå§‹è¯­ä¹‰å…³é”®è¯ï¼ˆä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ï¼‰
        original_concepts = extract_concepts(user_input)
        processed_concepts = extract_concepts(processed_query)

        # æ£€æŸ¥æ˜¯å¦ä¸¢å¤±äº†æ ¸å¿ƒæ¦‚å¿µï¼ˆå¦‚"é¼»å¡"å¯¹åº”"é¼»å­å µ"ï¼‰
        concept_loss = False
        for orig_concept in original_concepts:
            if not any(semantic_match(orig_concept, proc_concept) for proc_concept in processed_concepts):
                concept_loss = True
                break

        if concept_loss:
            print(f"âš ï¸ é¢„å¤„ç†å¤±è´¥ï¼šä¸¢å¤±äº†åŸå§‹æ¦‚å¿µã€‚åŸå§‹ï¼š{original_concepts}ï¼Œå¤„ç†åï¼š{processed_concepts}")
            return rule_based_preprocess(user_input)

        # éªŒè¯2ï¼šé•¿åº¦ä¸èƒ½å¤ªçŸ­ï¼ˆè‡³å°‘ä¿ç•™åŸæŸ¥è¯¢çš„ä¸€åŠé•¿åº¦ï¼‰
        if len(processed_query) < len(user_input) * 0.5:
            print(f"âš ï¸ é¢„å¤„ç†å¤±è´¥ï¼šè¾“å‡ºå¤ªçŸ­ã€‚åŸå§‹ï¼š{len(user_input)}å­—ç¬¦ï¼Œå¤„ç†åï¼š{len(processed_query)}å­—ç¬¦")
            return rule_based_preprocess(user_input)

        # éªŒè¯3ï¼šä¸èƒ½æ˜¯é€šç”¨çŸ­è¯­ï¼ˆé»‘åå•æ£€æŸ¥ï¼‰
        generic_phrases = ['åŒ»ç”Ÿå»ºè®®', 'åƒç‚¹ä»€ä¹ˆ', 'æ€ä¹ˆæ²»ç–—', 'æ€ä¹ˆåŠ', 'çœ‹åŒ»ç”Ÿ', 'å»åŒ»é™¢', 'æ²»ç–—å»ºè®®', 'å’¨è¯¢åŒ»ç”Ÿ']
        if any(phrase in processed_query for phrase in generic_phrases) and len(processed_query) < 20:
            print(f"âš ï¸ é¢„å¤„ç†å¤±è´¥ï¼šç”Ÿæˆäº†é€šç”¨çŸ­è¯­ã€‚è¾“å‡ºï¼š{processed_query}")
            return rule_based_preprocess(user_input)

        # éªŒè¯4ï¼šå¿…é¡»æœ‰åŒ»å­¦æœ¯è¯­
        if not has_medical_terms(processed_query):
            print(f"âš ï¸ é¢„å¤„ç†å¤±è´¥ï¼šæœªè¯†åˆ«åˆ°åŒ»å­¦æœ¯è¯­ã€‚è¾“å‡ºï¼š{processed_query}")
            return rule_based_preprocess(user_input)

        return processed_query

    except Exception as e:
        print(f"âš ï¸ é¢„å¤„ç†å¼‚å¸¸ï¼š{e}ï¼Œå›é€€åˆ°è§„åˆ™å¤„ç†")
        return rule_based_preprocess(user_input)


def rule_based_preprocess(user_input):
    # åŒ»å­¦æœ¯è¯­æ˜ å°„è¡¨ï¼ˆè¦†ç›–å¸¸è§ç—‡çŠ¶å’ŒæŸ¥è¯¢ï¼‰
    term_mapping = {
        # ç—‡çŠ¶
        'é¼»å­å µ': 'é¼»å¡',
        'é¼»å¡': 'é¼»å¡',
        'æµé¼»æ¶•': 'é¼»æº¢',
        'æµé¼»æ¶•': 'é¼»æº¢',
        'å‘çƒ§': 'å‘çƒ­',
        'å‘çƒ­': 'å‘çƒ­',
        'æ‹‰è‚šå­': 'è…¹æ³»',
        'è…¹æ³»': 'è…¹æ³»',
        'å¤´ç–¼': 'å¤´ç—›',
        'å¤´ç—›': 'å¤´ç—›',
        'å¤´æ™•': 'çœ©æ™•',
        'çœ©æ™•': 'çœ©æ™•',
        'å’³å—½': 'å’³å—½',
        'å‡ºè¡€': 'å‡ºè¡€',
        'å‡ºè¡€äº†': 'å‡ºè¡€',
        'ç—’': 'ç˜™ç—’',
        'ç˜™ç—’': 'ç˜™ç—’',
        'è‚¿': 'è‚¿èƒ€',
        'è‚¿èƒ€': 'è‚¿èƒ€',
        'ç—›': 'ç–¼ç—›',
        'ç–¼ç—›': 'ç–¼ç—›',

        # æ²»ç–—æŸ¥è¯¢
        'åƒè¯': 'è¯ç‰©æ²»ç–—',
        'ç”¨è¯': 'è¯ç‰©æ²»ç–—',
        'åƒä»€ä¹ˆè¯': 'è¯ç‰©æ²»ç–—',
        'è¯¥ç”¨ä»€ä¹ˆ': 'æ²»ç–—',
        'æ€ä¹ˆæ²»ç–—': 'æ²»ç–—',
        'æ€ä¹ˆåŠ': 'æ²»ç–—',
        'å’‹æ²»': 'æ²»ç–—',
        'å’‹æ•´': 'æ²»ç–—',
        'å’‹å¼„': 'æ²»ç–—',
        'å¦‚ä½•æ²»': 'æ²»ç–—',

        # ç–¾ç—…
        'æ„Ÿå†’': 'ä¸Šå‘¼å¸é“æ„ŸæŸ“',
        'é¼»ç‚': 'é¼»ç‚',
        'é¼»çª¦ç‚': 'é¼»çª¦ç‚',
        'è¿‡æ•': 'è¿‡æ•ååº”',
        'è‚ºç‚': 'è‚ºç‚',
        'èƒƒç‚': 'èƒƒç‚',
        'è‚ ç‚': 'è‚ ç‚',
    }

    # æå–åŸå§‹å…³é”®è¯
    keywords = []
    # å…ˆåŒ¹é…æœ€é•¿è¯ç»„
    for colloquial in sorted(term_mapping.keys(), key=len, reverse=True):
        if colloquial in user_input:
            keywords.append(term_mapping[colloquial])
            user_input = user_input.replace(colloquial, '')  # é¿å…é‡å¤åŒ¹é…

    # å»é‡
    keywords = list(dict.fromkeys(keywords))

    # å¦‚æœæœ‰å…³é”®è¯ï¼Œæ·»åŠ é€šç”¨åŒ»å­¦ç»´åº¦
    if keywords:
        if any(k in user_input for k in ['è¯', 'æ²»ç–—', 'æ€ä¹ˆåŠ', 'å’‹æ²»']):
            keywords.extend(['è¯Šæ–­', 'ç—…å› ', 'é¢„é˜²'])
        # é™åˆ¶æ•°é‡
        keywords = keywords[:5]

    result = " ".join(keywords)

    # ç¡®ä¿ä¸ä¸ºç©º
    if not result:
        # æœ€åæƒ…å†µï¼šè¿”å›åŸå§‹è¾“å…¥+é€šç”¨è¯
        result = user_input + " æ²»ç–— è¯Šæ–­"

    print(f"âœ… è§„åˆ™é¢„å¤„ç†æˆåŠŸï¼š{user_input} â†’ {result}")
    return result


def extract_concepts(text):
    """æå–æ–‡æœ¬ä¸­çš„æ ¸å¿ƒæ¦‚å¿µï¼ˆç”¨äºè¯­ä¹‰åŒ¹é…ï¼‰"""
    # ç§»é™¤æ ‡ç‚¹
    text = re.sub(r'[^\w\s]', '', text)
    # åˆ†è¯
    words = text.split()
    # ä¿ç•™åè¯æ€§è¯æ±‡ï¼ˆç®€åŒ–ï¼‰
    return [w for w in words if len(w) > 1]


def semantic_match(concept1, concept2):
    """æ£€æŸ¥ä¸¤ä¸ªæ¦‚å¿µæ˜¯å¦è¯­ä¹‰åŒ¹é…ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    # ç›´æ¥ç›¸ç­‰æˆ–åŒ…å«å…³ç³»
    if concept1 in concept2 or concept2 in concept1:
        return True

    # åŒä¹‰è¯æ˜ å°„
    synonyms = {
        'é¼»å­å µ': ['é¼»å¡', 'å µ', 'å µäº†'],
        'é¼»å¡': ['é¼»å­å µ', 'å µ'],
        'åƒè¯': ['è¯ç‰©', 'ç”¨è¯', 'æ²»ç–—', 'åƒä»€ä¹ˆè¯', 'è¯¥ç”¨ä»€ä¹ˆè¯'],
        'æ²»ç–—': ['ç”¨è¯', 'åƒè¯', 'æ²»ç–—', 'å’‹æ²»', 'æ€ä¹ˆåŠ'],
    }

    for key, values in synonyms.items():
        if concept1 in values and concept2 in [key] + values:
            return True
        if concept2 in values and concept1 in [key] + values:
            return True

    return False


def has_medical_terms(text):
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«åŒ»å­¦æœ¯è¯­"""
    medical_patterns = [
        r'\b\w*(?:ç‚|ç—‡|ç—…|ç˜¤|ç™Œ|å¾|ç•¸å½¢|æŸä¼¤|æ„ŸæŸ“|éšœç¢|åŠŸèƒ½ä¸å…¨|è¡°ç«­|å‡ºè¡€|æ¢—æ­»|æ “å¡|ç—›|ç—’|è‚¿|èƒ€|æ™•|éº»)\b',
        r'\b(?:è¯ç‰©|æ²»ç–—|è¯Šæ–­|ç—…å› |é¢„é˜²|å¹¶å‘ç—‡|æ‰‹æœ¯|æŠ¤ç†|åº·å¤|æ£€æŸ¥|ç–—æ³•|æ–¹æ¡ˆ)\w*\b',
    ]

    for pattern in medical_patterns:
        if re.search(pattern, text):
            return True

    return False


def generate_answer_stream(query, context_docs, gen_model, tokenizer):

    if not context_docs:
        yield "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
        return

    if not gen_model or not tokenizer:
        yield "âŒ ç”Ÿæˆç»„ä»¶æœªåŠ è½½ã€‚"
        return

    try:
        # æ”¹è¿›ä¸Šä¸‹æ–‡æ„å»º
        context_parts = []
        for i, doc in enumerate(context_docs[:3]):
            title = doc.get('title', 'æœªçŸ¥æ ‡é¢˜')
            content = doc.get('content', doc.get('abstract', ''))
            if content and len(content.strip()) > 50:
                content_preview = content[:1000] if len(content) > 1000 else content
                context_parts.append(f"æ–‡æ¡£{i + 1}ã€Š{title}ã€‹ï¼š\n{content_preview}")

        context = "\n\n---\n\n".join(context_parts)

        if not context or len(context.strip()) < 100:
            yield "âš ï¸ æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆç­”æ¡ˆã€‚è¯·å°è¯•æ›´å…·ä½“çš„é—®é¢˜ã€‚"
            return

        # å¢å¼ºæç¤ºè¯ï¼šæ˜ç¡®è¦æ±‚è¯¦ç»†å›ç­”
        prompt = f"""åŸºäºä»¥ä¸‹åŒ»å­¦æ–‡çŒ®ï¼Œè¯·è¯¦ç»†å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·æä¾›å®Œæ•´ã€å‡†ç¡®ä¸”æ˜“äºç†è§£çš„ç­”æ¡ˆã€‚

å‚è€ƒæ–‡çŒ®ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æä¾›ç®€æ´çš„åŒ»å­¦è§£ç­”ï¼š
"""

        inputs = tokenizer(prompt, return_tensors="pt").to(gen_model.device)
        input_length = inputs['input_ids'].shape[1]

        gen_model.eval()
        if hasattr(gen_model, 'generation_config'):
            gen_model.generation_config.output_scores = False

        past_key_values = None
        current_tokens = inputs['input_ids']

        # å®æ—¶è§£ç ï¼Œé¿å…ç´¯ç§¯å¯¼è‡´çš„æå‰ç»ˆæ­¢
        min_length = 50  # æœ€å°‘ç”Ÿæˆ50ä¸ªtoken

        for step in range(MAX_NEW_TOKENS_GEN):
            with torch.no_grad():
                if past_key_values is None:
                    outputs = gen_model(current_tokens, use_cache=True)
                else:
                    outputs = gen_model(current_tokens[:, -1:], past_key_values=past_key_values, use_cache=True)

                logits = outputs.logits[:, -1, :]
                past_key_values = outputs.past_key_values

                # è°ƒæ•´é‡‡æ ·å‚æ•°
                next_token_logits = logits / (TEMPERATURE * 0.6)
                next_token = torch.multinomial(torch.softmax(next_token_logits, dim=-1), num_samples=1)

                # åªæœ‰åœ¨ç”Ÿæˆè¶³å¤Ÿå†…å®¹åæ‰å…è®¸EOS
                if next_token.item() == tokenizer.eos_token_id and step > min_length:
                    break

                current_tokens = torch.cat([current_tokens, next_token], dim=-1)

                # å®æ—¶è§£ç å¹¶è¾“å‡º
                new_text = tokenizer.decode(next_token[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)

                # è½¬æ¢ä¸º UTF-8 ç¼–ç ä»¥ç¡®ä¿æµè¾“å‡ºä¸­æ–‡æ—¶æ²¡æœ‰ä¹±ç 
                if new_text and new_text.isprintable() and not new_text.isspace():
                    yield new_text

    except Exception as e:
        yield f"ç”Ÿæˆé”™è¯¯: {e}"
        yield "\nğŸ’¡ å»ºè®®ï¼šè¯·æ£€æŸ¥æ¨¡å‹çŠ¶æ€æˆ–é‡æ–°å¯åŠ¨åº”ç”¨ã€‚"

